[package]
name = "siumai"
version = "0.9.1"
edition = "2024"
authors = ["Mingzhen Zhuang <superfrankie621@gmail.com>"]
description = "A unified LLM interface library for Rust"
license = "MIT OR Apache-2.0"
repository = "https://github.com/YumchaLabs/siumai"
homepage = "https://github.com/YumchaLabs/siumai"
documentation = "https://docs.rs/siumai"
readme = "README.md"
keywords = ["llm", "ai", "openai", "anthropic", "async"]
categories = ["api-bindings"]

exclude = [
    "docs/*",
]

[dependencies]
# Async traits
async-trait = { workspace = true }

# Serialization
serde = { workspace = true, features = ["derive"] }
serde_json = { workspace = true }

# Validation
validator = { version = "0.20", features = ["derive"] }

# HTTP client
reqwest = { workspace = true, features = ["json", "stream", "multipart"] }
reqwest-eventsource = "0.6"

# Async runtime
tokio = { workspace = true, features = ["full"] }
futures = { workspace = true }
async-stream = "0.3"

# Streaming
futures-util = { workspace = true }
pin-project-lite = { workspace = true }
bytes = { workspace = true }
eventsource-stream = "0.2"

# Time handling
chrono = { workspace = true, features = ["serde"] }

# Error handling
thiserror = { workspace = true }

# Static assertions
static_assertions = "1.1"

# UUID generation
uuid = { workspace = true, features = ["v4", "serde"] }

# Tracing and observability
tracing = { workspace = true }
tracing-subscriber = { workspace = true, features = ["env-filter", "json", "fmt", "time", "chrono"] }
tracing-appender = "0.2"

# Random number generation (for retry jitter)
rand = { workspace = true }

# URL encoding
urlencoding = "2.1"
regex = { workspace = true }

# Security
secrecy = { version = "0.10", features = ["serde"] }

# Retry mechanism
backoff = { version = "0.4", features = ["tokio"] }

[features]
# Default features - include all providers for convenience
default = ["all-providers"]

# Provider features - each provider can be enabled independently
openai = []
anthropic = []
google = []
ollama = []
xai = []
groq = []

# Convenience features for common combinations
all-providers = ["openai", "anthropic", "google", "ollama", "xai", "groq"]

[dev-dependencies]
tokio-test = "0.4"
mockito = "1.0"
wiremock = "0.6"
tracing-test = "0.2"
tempfile = "3.0"
futures-util = "0.3"
rmcp = { version = "0.6", features = ["transport-io", "transport-child-process", "macros", "server", "client"] }
serde = { version = "1.0", features = ["derive"] }
chrono = { version = "0.4", features = ["serde"] }
axum = "0.8.4"

# Examples configuration
[[example]]
name = "quick_start"
path = "examples/01_getting_started/quick_start.rs"

[[example]]
name = "provider_comparison"
path = "examples/01_getting_started/provider_comparison.rs"

[[example]]
name = "chat_basics"
path = "examples/02_core_features/chat_basics.rs"

[[example]]
name = "streaming_chat"
path = "examples/02_core_features/streaming_chat.rs"

[[example]]
name = "unified_interface"
path = "examples/02_core_features/unified_interface.rs"

[[example]]
name = "error_handling"
path = "examples/02_core_features/error_handling.rs"

[[example]]
name = "capability_detection"
path = "examples/02_core_features/capability_detection.rs"

[[example]]
name = "embedding"
path = "examples/02_core_features/embedding.rs"

[[example]]
name = "thinking_models"
path = "examples/03_advanced_features/thinking_models.rs"

[[example]]
name = "thinking_content_processing"
path = "examples/03_advanced_features/thinking_content_processing.rs"

[[example]]
name = "batch_processing"
path = "examples/03_advanced_features/batch_processing.rs"

[[example]]
name = "custom_provider"
path = "examples/03_advanced_features/custom_provider.rs"

[[example]]
name = "tracing_monitoring"
path = "examples/03_advanced_features/tracing_monitoring.rs"

[[example]]
name = "custom_configurations"
path = "examples/03_advanced_features/custom_configurations.rs"

[[example]]
name = "multimodal_processing"
path = "examples/03_advanced_features/multimodal_processing.rs"

[[example]]
name = "openai_response_api_http_mcp_client"
path = "examples/04_providers/openai/openai_response_api_http_mcp_client.rs"

[[example]]
name = "openai_basic_chat"
path = "examples/04_providers/openai/basic_chat.rs"

[[example]]
name = "simple_chatbot"
path = "examples/05_use_cases/simple_chatbot.rs"

[[example]]
name = "convenience_methods"
path = "examples/01_getting_started/convenience_methods.rs"

[[example]]
name = "anthropic_basic_chat"
path = "examples/04_providers/anthropic/basic_chat.rs"

[[example]]
name = "anthropic_thinking_showcase"
path = "examples/04_providers/anthropic/thinking_showcase.rs"

[[example]]
name = "google_basic_usage"
path = "examples/04_providers/google/basic_usage.rs"

[[example]]
name = "ollama_basic_setup"
path = "examples/04_providers/ollama/basic_setup.rs"

[[example]]
name = "ollama_thinking_and_tools"
path = "examples/04_providers/ollama/thinking_and_tools.rs"

[[example]]
name = "openai_enhanced_features"
path = "examples/04_providers/openai/enhanced_features.rs"

[[example]]
name = "openai_vision_processing"
path = "examples/04_providers/openai/vision_processing.rs"

[[example]]
name = "openai_compatible_models_showcase"
path = "examples/04_providers/openai_compatible/models_showcase.rs"

[[example]]
name = "openai_responses_api"
path = "examples/04_providers/openai/openai_responses_api.rs"

[[example]]
name = "api_integration"
path = "examples/05_use_cases/api_integration.rs"

[[example]]
name = "code_assistant"
path = "examples/05_use_cases/code_assistant.rs"

[[example]]
name = "content_generator"
path = "examples/05_use_cases/content_generator.rs"

[[example]]
name = "response_cache"
path = "examples/02_core_features/response_cache.rs"

[[example]]
name = "http_mcp_client"
path = "examples/06_mcp_integration/http_mcp_client.rs"

[[example]]
name = "http_mcp_server"
path = "examples/06_mcp_integration/http_mcp_server.rs"

[[example]]
name = "stdio_mcp_client"
path = "examples/06_mcp_integration/stdio_mcp_client.rs"

[[example]]
name = "stdio_mcp_server"
path = "examples/06_mcp_integration/stdio_mcp_server.rs"






