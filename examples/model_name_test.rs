//! æµ‹è¯•æ‰€æœ‰æä¾›å•†æ˜¯å¦æ­£ç¡®ä½¿ç”¨ siumai builder çš„æ¨¡å‹åå’Œå…¶ä»–å‚æ•°
//!
//! è¿™ä¸ªç¤ºä¾‹æ¼”ç¤ºäº†æ‰€æœ‰æä¾›å•†ç°åœ¨æ­£ç¡®ä½¿ç”¨ä» builder è®¾ç½®çš„æ¨¡å‹åå’Œå…¶ä»–å‚æ•°ã€‚

use siumai::prelude::*;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // åˆ›å»ºä¸€ä¸ª OpenAI å®¢æˆ·ç«¯ï¼ŒæŒ‡å®šæ¨¡å‹å
    let client = LlmBuilder::new()
        .openai()
        .api_key("test-key") // è¿™é‡Œä½¿ç”¨æµ‹è¯•å¯†é’¥ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦çœŸå®çš„ API å¯†é’¥
        .model("gpt-4-turbo") // è®¾ç½®æ¨¡å‹å
        .build()
        .await?;

    // éªŒè¯å®¢æˆ·ç«¯æ˜¯å¦å­˜å‚¨äº†æ­£ç¡®çš„æ¨¡å‹å
    println!(
        "âœ… OpenAI å®¢æˆ·ç«¯å·²åˆ›å»ºï¼Œæ¨¡å‹å: {}",
        client.common_params().model
    );

    // åˆ›å»ºä¸€ä¸ªæµ‹è¯•æ¶ˆæ¯
    let message = ChatMessage::user("Hello, world!");

    // åˆ›å»ºä¸€ä¸ª ChatRequest æ¥æµ‹è¯•æ¨¡å‹åçš„ä½¿ç”¨
    let request = ChatRequest {
        messages: vec![message.build()],
        tools: None,
        common_params: client.common_params().clone(),
        provider_params: None,
        http_config: None,
        web_search: None,
        stream: false,
    };

    // æµ‹è¯•è¯·æ±‚ä½“æ˜¯å¦åŒ…å«æ­£ç¡®çš„æ¨¡å‹å
    let body = client.chat_capability().build_chat_request_body(&request)?;

    if let Some(model) = body.get("model") {
        println!("âœ… è¯·æ±‚ä½“ä¸­çš„æ¨¡å‹å: {}", model);
        assert_eq!(model, "gpt-4-turbo");
        println!("âœ… æ¨¡å‹åéªŒè¯æˆåŠŸï¼OpenAI æ­£ç¡®ä½¿ç”¨äº† siumai builder çš„æ¨¡å‹åã€‚");
    } else {
        println!("âŒ è¯·æ±‚ä½“ä¸­æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹å");
        return Err("è¯·æ±‚ä½“ä¸­æ²¡æœ‰æ¨¡å‹å".into());
    }

    // æµ‹è¯•ä¸åŒçš„æ¨¡å‹å
    println!("\næµ‹è¯•ä¸åŒçš„æ¨¡å‹å...");

    let client2 = LlmBuilder::new()
        .openai()
        .api_key("test-key")
        .model("gpt-3.5-turbo") // ä¸åŒçš„æ¨¡å‹å
        .build()
        .await?;

    let request2 = ChatRequest {
        messages: vec![ChatMessage::user("Test").build()],
        tools: None,
        common_params: client2.common_params().clone(),
        provider_params: None,
        http_config: None,
        web_search: None,
        stream: false,
    };

    let body2 = client2
        .chat_capability()
        .build_chat_request_body(&request2)?;

    if let Some(model) = body2.get("model") {
        println!("âœ… ç¬¬äºŒä¸ªå®¢æˆ·ç«¯çš„æ¨¡å‹å: {}", model);
        assert_eq!(model, "gpt-3.5-turbo");
        println!("âœ… ç¬¬äºŒä¸ªæ¨¡å‹åéªŒè¯æˆåŠŸï¼");
    }

    // æµ‹è¯• Anthropic æä¾›å•†
    println!("\næµ‹è¯• Anthropic æä¾›å•†...");

    let anthropic_client = LlmBuilder::new()
        .anthropic()
        .api_key("test-key")
        .model("claude-3-5-sonnet-20241022") // è®¾ç½®æ¨¡å‹å
        .temperature(0.8) // è®¾ç½®æ¸©åº¦
        .max_tokens(2000) // è®¾ç½®æœ€å¤§ä»¤ç‰Œæ•°
        .build()
        .await?;

    println!(
        "âœ… Anthropic å®¢æˆ·ç«¯å·²åˆ›å»ºï¼Œæ¨¡å‹å: {}",
        anthropic_client.common_params().model
    );
    println!(
        "âœ… Anthropic å®¢æˆ·ç«¯æ¸©åº¦: {:?}",
        anthropic_client.common_params().temperature
    );
    println!(
        "âœ… Anthropic å®¢æˆ·ç«¯æœ€å¤§ä»¤ç‰Œæ•°: {:?}",
        anthropic_client.common_params().max_tokens
    );

    let anthropic_request = ChatRequest {
        messages: vec![ChatMessage::user("Test").build()],
        tools: None,
        common_params: anthropic_client.common_params().clone(),
        provider_params: None,
        http_config: None,
        web_search: None,
        stream: false,
    };

    let anthropic_body = anthropic_client
        .chat_capability()
        .build_chat_request_body(&anthropic_request, Some(anthropic_client.specific_params()))?;

    if let Some(model) = anthropic_body.get("model") {
        println!("âœ… Anthropic è¯·æ±‚ä½“ä¸­çš„æ¨¡å‹å: {}", model);
        assert_eq!(model, "claude-3-5-sonnet-20241022");
    }

    if let Some(temperature) = anthropic_body.get("temperature") {
        println!("âœ… Anthropic è¯·æ±‚ä½“ä¸­çš„æ¸©åº¦: {}", temperature);
        // Use approximate comparison for floating point values
        let temp_val = temperature.as_f64().unwrap();
        assert!((temp_val - 0.8).abs() < 1e-6);
    }

    if let Some(max_tokens) = anthropic_body.get("max_tokens") {
        println!("âœ… Anthropic è¯·æ±‚ä½“ä¸­çš„æœ€å¤§ä»¤ç‰Œæ•°: {}", max_tokens);
        assert_eq!(max_tokens, 2000);
    }

    // æµ‹è¯• Ollama æä¾›å•†
    println!("\næµ‹è¯• Ollama æä¾›å•†...");

    let ollama_client = LlmBuilder::new()
        .ollama()
        .base_url("http://localhost:11434")
        .model("llama3.2") // è®¾ç½®æ¨¡å‹å
        .temperature(0.9) // è®¾ç½®æ¸©åº¦
        .max_tokens(1500) // è®¾ç½®æœ€å¤§ä»¤ç‰Œæ•°
        .build()
        .await?;

    println!(
        "âœ… Ollama å®¢æˆ·ç«¯å·²åˆ›å»ºï¼Œæ¨¡å‹å: {}",
        ollama_client.common_params().model
    );
    println!(
        "âœ… Ollama å®¢æˆ·ç«¯æ¸©åº¦: {:?}",
        ollama_client.common_params().temperature
    );
    println!(
        "âœ… Ollama å®¢æˆ·ç«¯æœ€å¤§ä»¤ç‰Œæ•°: {:?}",
        ollama_client.common_params().max_tokens
    );

    let ollama_request = ChatRequest {
        messages: vec![ChatMessage::user("Test").build()],
        tools: None,
        common_params: ollama_client.common_params().clone(),
        provider_params: None,
        http_config: None,
        web_search: None,
        stream: false,
    };

    let ollama_body = ollama_client
        .chat_capability()
        .build_chat_request_body(&ollama_request)?;

    println!("âœ… Ollama è¯·æ±‚ä½“ä¸­çš„æ¨¡å‹å: {}", ollama_body.model);
    assert_eq!(ollama_body.model, "llama3.2");

    if let Some(options) = &ollama_body.options {
        if let Some(temperature) = options.get("temperature") {
            println!("âœ… Ollama è¯·æ±‚ä½“ä¸­çš„æ¸©åº¦: {}", temperature);
            let temp_val = temperature.as_f64().unwrap();
            assert!((temp_val - 0.9).abs() < 1e-6);
        }
        if let Some(num_predict) = options.get("num_predict") {
            println!("âœ… Ollama è¯·æ±‚ä½“ä¸­çš„æœ€å¤§ä»¤ç‰Œæ•°: {}", num_predict);
            assert_eq!(num_predict, 1500);
        }
    }

    // æµ‹è¯• Gemini æä¾›å•†
    println!("\næµ‹è¯• Gemini æä¾›å•†...");

    let gemini_client = LlmBuilder::new()
        .gemini()
        .api_key("test-key")
        .model("gemini-1.5-pro") // è®¾ç½®æ¨¡å‹å
        .temperature(0.6) // è®¾ç½®æ¸©åº¦
        .max_tokens(3000) // è®¾ç½®æœ€å¤§ä»¤ç‰Œæ•°
        .build()
        .await?;

    println!(
        "âœ… Gemini å®¢æˆ·ç«¯å·²åˆ›å»ºï¼Œæ¨¡å‹å: {}",
        gemini_client.config().model
    );
    println!(
        "âœ… Gemini å®¢æˆ·ç«¯æ¸©åº¦: {:?}",
        gemini_client
            .config()
            .generation_config
            .as_ref()
            .and_then(|gc| gc.temperature)
    );
    println!(
        "âœ… Gemini å®¢æˆ·ç«¯æœ€å¤§ä»¤ç‰Œæ•°: {:?}",
        gemini_client
            .config()
            .generation_config
            .as_ref()
            .and_then(|gc| gc.max_output_tokens)
    );

    let gemini_request = gemini_client
        .chat_capability()
        .build_request_body(&[ChatMessage::user("Test").build()], None)?;

    println!("âœ… Gemini è¯·æ±‚ä½“ä¸­çš„æ¨¡å‹å: {}", gemini_request.model);
    assert_eq!(gemini_request.model, "gemini-1.5-pro");

    if let Some(generation_config) = &gemini_request.generation_config {
        if let Some(temperature) = generation_config.temperature {
            println!("âœ… Gemini è¯·æ±‚ä½“ä¸­çš„æ¸©åº¦: {}", temperature);
            assert!((temperature - 0.6).abs() < 1e-6);
        }
        if let Some(max_output_tokens) = generation_config.max_output_tokens {
            println!("âœ… Gemini è¯·æ±‚ä½“ä¸­çš„æœ€å¤§ä»¤ç‰Œæ•°: {}", max_output_tokens);
            assert_eq!(max_output_tokens, 3000);
        }
    }

    println!("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ‰€æœ‰æä¾›å•†ç°åœ¨éƒ½æ­£ç¡®ä½¿ç”¨ siumai builder çš„æ¨¡å‹åå’Œå…¶ä»–å‚æ•°ã€‚");

    Ok(())
}
