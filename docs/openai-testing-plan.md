# OpenAI API æµ‹è¯•è®¡åˆ’

## æ¦‚è¿°

æœ¬æ–‡æ¡£å®šä¹‰äº† OpenAI API åˆè§„æ€§ä¿®å¤çš„å®Œæ•´æµ‹è¯•ç­–ç•¥ï¼Œç¡®ä¿æˆ‘ä»¬çš„å®ç°ä¸å®˜æ–¹ OpenAPI è§„èŒƒ (`docs/openapi.documented.yml`) å®Œå…¨å…¼å®¹ã€‚

## ğŸ§ª æµ‹è¯•ç­–ç•¥

### æµ‹è¯•å±‚çº§

1. **å•å…ƒæµ‹è¯•** - æµ‹è¯•å•ä¸ªå‡½æ•°å’Œæ–¹æ³•
2. **é›†æˆæµ‹è¯•** - æµ‹è¯•ä¸çœŸå® OpenAI API çš„äº¤äº’
3. **åˆè§„æ€§æµ‹è¯•** - éªŒè¯ä¸ OpenAPI è§„èŒƒçš„ä¸€è‡´æ€§
4. **å›å½’æµ‹è¯•** - ç¡®ä¿æ–°åŠŸèƒ½ä¸ç ´åç°æœ‰åŠŸèƒ½

### æµ‹è¯•ç¯å¢ƒ

- **æ¨¡æ‹Ÿç¯å¢ƒ**: ä½¿ç”¨ mock æœåŠ¡å™¨è¿›è¡Œå¿«é€Ÿæµ‹è¯•
- **æ²™ç›’ç¯å¢ƒ**: ä½¿ç”¨ OpenAI æµ‹è¯• API å¯†é’¥
- **ç”Ÿäº§ç¯å¢ƒ**: ä½¿ç”¨çœŸå® API è¿›è¡Œæœ€ç»ˆéªŒè¯

## ğŸ“‹ Chat Completions API æµ‹è¯•

### 1. æ¶ˆæ¯è§’è‰²æµ‹è¯•

**æµ‹è¯•ç›®æ ‡**: éªŒè¯æ–°çš„ `developer` è§’è‰²æ”¯æŒ

```rust
#[cfg(test)]
mod chat_role_tests {
    use super::*;
    
    #[test]
    fn test_developer_role_serialization() {
        let message = ChatMessage {
            role: ChatRole::Developer,
            content: "You are a helpful assistant.".to_string(),
        };
        
        let json = serde_json::to_string(&message).unwrap();
        assert!(json.contains("\"role\":\"developer\""));
    }
    
    #[test]
    fn test_developer_role_deserialization() {
        let json = r#"{"role":"developer","content":"test"}"#;
        let message: ChatMessage = serde_json::from_str(json).unwrap();
        assert_eq!(message.role, ChatRole::Developer);
    }
    
    #[tokio::test]
    async fn test_developer_role_api_call() {
        let client = create_test_client().await;
        let messages = vec![
            ChatMessage {
                role: ChatRole::Developer,
                content: "You are a helpful assistant.".to_string(),
            },
            ChatMessage {
                role: ChatRole::User,
                content: "Hello!".to_string(),
            },
        ];
        
        let response = client.chat_with_tools(messages, None).await;
        assert!(response.is_ok());
    }
}
```

### 2. æ–°å‚æ•°æµ‹è¯•

**æµ‹è¯•ç›®æ ‡**: éªŒè¯æ‰€æœ‰æ–°å¢çš„ Chat API å‚æ•°

```rust
#[cfg(test)]
mod chat_parameters_tests {
    use super::*;
    
    #[test]
    fn test_reasoning_effort_parameter() {
        let request = OpenAiChatRequest {
            model: "o1-preview".to_string(),
            messages: vec![test_message()],
            reasoning_effort: Some(ReasoningEffort::High),
            ..Default::default()
        };
        
        let json = serde_json::to_string(&request).unwrap();
        assert!(json.contains("\"reasoning_effort\":\"high\""));
    }
    
    #[test]
    fn test_frequency_penalty_validation() {
        let mut request = OpenAiChatRequest::default();
        
        // æœ‰æ•ˆèŒƒå›´
        request.frequency_penalty = Some(1.5);
        assert!(request.validate().is_ok());
        
        // æ— æ•ˆèŒƒå›´
        request.frequency_penalty = Some(3.0);
        assert!(request.validate().is_err());
        
        request.frequency_penalty = Some(-3.0);
        assert!(request.validate().is_err());
    }
    
    #[test]
    fn test_presence_penalty_validation() {
        let mut request = OpenAiChatRequest::default();
        
        // æœ‰æ•ˆèŒƒå›´
        request.presence_penalty = Some(-1.0);
        assert!(request.validate().is_ok());
        
        // æ— æ•ˆèŒƒå›´
        request.presence_penalty = Some(2.5);
        assert!(request.validate().is_err());
    }
    
    #[tokio::test]
    async fn test_max_completion_tokens() {
        let client = create_test_client().await;
        let request = ChatRequest {
            messages: vec![test_message()],
            max_completion_tokens: Some(100),
            ..Default::default()
        };
        
        let response = client.chat_with_request(request).await;
        assert!(response.is_ok());
        
        // éªŒè¯å“åº”ä¸­çš„ token ä½¿ç”¨æƒ…å†µ
        let chat_response = response.unwrap();
        if let Some(usage) = chat_response.usage {
            assert!(usage.completion_tokens <= 100);
        }
    }
}
```

### 3. æ¨ç†æ¨¡å‹å…¼å®¹æ€§æµ‹è¯•

```rust
#[cfg(test)]
mod reasoning_model_tests {
    use super::*;
    
    #[test]
    fn test_reasoning_model_parameter_restrictions() {
        let mut request = OpenAiChatRequest {
            model: "o1-preview".to_string(),
            reasoning_effort: Some(ReasoningEffort::High),
            temperature: Some(0.7), // ä¸åº”è¯¥è¢«å…è®¸
            ..Default::default()
        };
        
        // æ¨ç†æ¨¡å‹ä¸åº”è¯¥æ”¯æŒ temperature
        assert!(request.validate().is_err());
        
        request.temperature = None;
        request.top_p = Some(0.9); // ä¹Ÿä¸åº”è¯¥è¢«å…è®¸
        assert!(request.validate().is_err());
    }
    
    #[tokio::test]
    async fn test_reasoning_model_response_format() {
        let client = create_test_client().await;
        let messages = vec![ChatMessage {
            role: ChatRole::User,
            content: "Solve this math problem: 2+2=?".to_string(),
        }];
        
        let response = client.chat_with_reasoning("o1-preview", messages, ReasoningEffort::High).await;
        assert!(response.is_ok());
        
        let chat_response = response.unwrap();
        // éªŒè¯æ¨ç†å†…å®¹æ˜¯å¦å­˜åœ¨
        assert!(chat_response.thinking.is_some());
    }
}
```

## ğŸµ Audio API æµ‹è¯•

### 1. TTS æ–°åŠŸèƒ½æµ‹è¯•

```rust
#[cfg(test)]
mod tts_tests {
    use super::*;
    
    #[test]
    fn test_new_tts_model_support() {
        let request = OpenAiTtsRequest {
            model: "gpt-4o-mini-tts".to_string(),
            input: "Hello world".to_string(),
            voice: "alloy".to_string(),
            instructions: Some("Speak slowly and clearly".to_string()),
            ..Default::default()
        };
        
        assert!(request.validate().is_ok());
    }
    
    #[test]
    fn test_instructions_model_compatibility() {
        // instructions ä¸åº”è¯¥ç”¨äºæ—§æ¨¡å‹
        let request = OpenAiTtsRequest {
            model: "tts-1".to_string(),
            input: "Hello".to_string(),
            voice: "alloy".to_string(),
            instructions: Some("Speak slowly".to_string()),
            ..Default::default()
        };
        
        assert!(request.validate().is_err());
    }
    
    #[test]
    fn test_new_voices() {
        let new_voices = vec!["ash", "ballad", "coral", "sage", "verse"];
        
        for voice in new_voices {
            let voice_enum: Result<TtsVoice, _> = serde_json::from_str(&format!("\"{}\"", voice));
            assert!(voice_enum.is_ok(), "Voice {} should be supported", voice);
        }
    }
    
    #[tokio::test]
    async fn test_tts_with_instructions() {
        let client = create_test_audio_client().await;
        let request = TtsRequest {
            text: "Hello, this is a test.".to_string(),
            model: Some("gpt-4o-mini-tts".to_string()),
            voice: Some("nova".to_string()),
            instructions: Some("Speak in a cheerful tone".to_string()),
            ..Default::default()
        };
        
        let response = client.text_to_speech(request).await;
        assert!(response.is_ok());
        
        let tts_response = response.unwrap();
        assert!(!tts_response.audio_data.is_empty());
    }
}
```

### 2. STT æµå¼æµ‹è¯•

```rust
#[cfg(test)]
mod stt_streaming_tests {
    use super::*;
    use futures::StreamExt;
    
    #[tokio::test]
    async fn test_streaming_transcription() {
        let client = create_test_audio_client().await;
        let audio_data = load_test_audio_file("test_audio.mp3");
        
        let request = TranscriptionRequest {
            file: audio_data,
            model: "gpt-4o-transcribe".to_string(),
            stream: Some(true),
            ..Default::default()
        };
        
        let mut stream = client.transcribe_stream(request).await.unwrap();
        let mut transcript_parts = Vec::new();
        
        while let Some(event) = stream.next().await {
            match event.unwrap() {
                TranscriptionEvent::TextDelta { text } => {
                    transcript_parts.push(text);
                }
                TranscriptionEvent::Complete { final_text } => {
                    assert!(!final_text.is_empty());
                    break;
                }
            }
        }
        
        assert!(!transcript_parts.is_empty());
    }
}
```

## ğŸ–¼ï¸ Images API æµ‹è¯•

### 1. æ–°æ¨¡å‹æµ‹è¯•

```rust
#[cfg(test)]
mod image_model_tests {
    use super::*;
    
    #[test]
    fn test_gpt_image_1_prompt_length() {
        let long_prompt = "A".repeat(32000);
        let request = ImageGenerationRequest {
            prompt: long_prompt,
            model: Some("gpt-image-1".to_string()),
            ..Default::default()
        };
        
        assert!(request.validate().is_ok());
        
        // è¶…è¿‡é™åˆ¶åº”è¯¥å¤±è´¥
        let too_long_prompt = "A".repeat(32001);
        let invalid_request = ImageGenerationRequest {
            prompt: too_long_prompt,
            model: Some("gpt-image-1".to_string()),
            ..Default::default()
        };
        
        assert!(invalid_request.validate().is_err());
    }
    
    #[tokio::test]
    async fn test_gpt_image_1_generation() {
        let client = create_test_images_client().await;
        let request = ImageGenerationRequest {
            prompt: "A futuristic cityscape with flying cars and neon lights".to_string(),
            model: Some("gpt-image-1".to_string()),
            size: Some("1024x1024".to_string()),
            count: 1,
            ..Default::default()
        };
        
        let response = client.generate_images(request).await;
        assert!(response.is_ok());
        
        let image_response = response.unwrap();
        assert_eq!(image_response.images.len(), 1);
    }
}
```

### 2. å›¾åƒç¼–è¾‘æµ‹è¯•

```rust
#[cfg(test)]
mod image_editing_tests {
    use super::*;
    
    #[test]
    fn test_edit_request_validation() {
        let image_data = load_test_image("test_image.png");
        let mask_data = load_test_image("test_mask.png");
        
        let request = ImageEditRequest {
            image: image_data,
            mask: Some(mask_data),
            prompt: "Add a red car to the scene".to_string(),
            model: Some(ImageModel::DallE2),
            ..Default::default()
        };
        
        assert!(request.validate().is_ok());
        
        // æµ‹è¯•ä¸æ”¯æŒç¼–è¾‘çš„æ¨¡å‹
        let invalid_request = ImageEditRequest {
            image: load_test_image("test_image.png"),
            prompt: "Edit this".to_string(),
            model: Some(ImageModel::DallE3), // ä¸æ”¯æŒç¼–è¾‘
            ..Default::default()
        };
        
        assert!(invalid_request.validate().is_err());
    }
    
    #[tokio::test]
    async fn test_image_editing_api() {
        let client = create_test_images_client().await;
        let image_data = load_test_image("test_image.png");
        
        let request = ImageEditRequest {
            image: image_data,
            prompt: "Add a blue sky background".to_string(),
            model: Some(ImageModel::DallE2),
            n: Some(1),
            ..Default::default()
        };
        
        let response = client.edit_image(request).await;
        assert!(response.is_ok());
        
        let edit_response = response.unwrap();
        assert!(!edit_response.images.is_empty());
    }
}
```

## ğŸ”§ åˆè§„æ€§æµ‹è¯•

### OpenAPI è§„èŒƒéªŒè¯

```rust
#[cfg(test)]
mod openapi_compliance_tests {
    use super::*;
    
    #[test]
    fn test_chat_request_schema_compliance() {
        // éªŒè¯è¯·æ±‚ç»“æ„ä¸ OpenAPI schema åŒ¹é…
        let request = OpenAiChatRequest {
            model: "gpt-4".to_string(),
            messages: vec![test_message()],
            frequency_penalty: Some(0.5),
            presence_penalty: Some(-0.5),
            max_completion_tokens: Some(1000),
            reasoning_effort: Some(ReasoningEffort::Medium),
            ..Default::default()
        };
        
        let json = serde_json::to_value(&request).unwrap();
        
        // éªŒè¯å¿…éœ€å­—æ®µå­˜åœ¨
        assert!(json["model"].is_string());
        assert!(json["messages"].is_array());
        
        // éªŒè¯å¯é€‰å­—æ®µæ ¼å¼
        assert!(json["frequency_penalty"].is_number());
        assert!(json["presence_penalty"].is_number());
        assert!(json["max_completion_tokens"].is_number());
        assert!(json["reasoning_effort"].is_string());
    }
    
    #[test]
    fn test_response_format_compliance() {
        // éªŒè¯å“åº”æ ¼å¼ä¸ OpenAPI schema åŒ¹é…
        let response_json = r#"{
            "id": "chatcmpl-123",
            "object": "chat.completion",
            "created": 1677652288,
            "model": "gpt-4",
            "choices": [{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": "Hello!"
                },
                "finish_reason": "stop"
            }],
            "usage": {
                "prompt_tokens": 9,
                "completion_tokens": 12,
                "total_tokens": 21
            }
        }"#;
        
        let response: ChatResponse = serde_json::from_str(response_json).unwrap();
        assert!(response.text().is_some());
        assert!(response.usage.is_some());
    }
}
```

## ğŸ“Š æ€§èƒ½æµ‹è¯•

```rust
#[cfg(test)]
mod performance_tests {
    use super::*;
    use std::time::Instant;
    
    #[tokio::test]
    async fn test_chat_response_time() {
        let client = create_test_client().await;
        let messages = vec![test_message()];
        
        let start = Instant::now();
        let response = client.chat_with_tools(messages, None).await;
        let duration = start.elapsed();
        
        assert!(response.is_ok());
        assert!(duration.as_secs() < 30); // å“åº”æ—¶é—´åº”è¯¥åœ¨ 30 ç§’å†…
    }
    
    #[tokio::test]
    async fn test_streaming_latency() {
        let client = create_test_client().await;
        let messages = vec![test_message()];
        
        let start = Instant::now();
        let mut stream = client.chat_stream(messages, None).await.unwrap();
        
        // æµ‹è¯•ç¬¬ä¸€ä¸ª token çš„å»¶è¿Ÿ
        if let Some(first_event) = stream.next().await {
            let first_token_latency = start.elapsed();
            assert!(first_token_latency.as_secs() < 5); // ç¬¬ä¸€ä¸ª token åº”è¯¥åœ¨ 5 ç§’å†…
        }
    }
}
```

## ğŸš€ é›†æˆæµ‹è¯•å¥—ä»¶

```rust
#[cfg(test)]
mod integration_tests {
    use super::*;
    
    #[tokio::test]
    async fn test_end_to_end_chat_workflow() {
        let client = create_test_client().await;
        
        // 1. åŸºæœ¬å¯¹è¯
        let messages = vec![
            ChatMessage {
                role: ChatRole::Developer,
                content: "You are a helpful math tutor.".to_string(),
            },
            ChatMessage {
                role: ChatRole::User,
                content: "What is 2+2?".to_string(),
            },
        ];
        
        let response = client.chat_with_tools(messages.clone(), None).await.unwrap();
        assert!(response.text().unwrap().contains("4"));
        
        // 2. å¸¦å·¥å…·çš„å¯¹è¯
        let tools = vec![create_calculator_tool()];
        let tool_response = client.chat_with_tools(messages, Some(tools)).await.unwrap();
        assert!(tool_response.tool_calls.is_some() || tool_response.text().is_some());
        
        // 3. æµå¼å¯¹è¯
        let mut stream = client.chat_stream(vec![test_message()], None).await.unwrap();
        let mut content = String::new();
        
        while let Some(event) = stream.next().await {
            match event.unwrap() {
                ChatStreamEvent::TextDelta { text } => content.push_str(&text),
                ChatStreamEvent::Complete { .. } => break,
                _ => {}
            }
        }
        
        assert!(!content.is_empty());
    }
}
```

## ğŸ“‹ æµ‹è¯•æ‰§è¡Œè®¡åˆ’

### é˜¶æ®µ 1: å•å…ƒæµ‹è¯• (1 å‘¨)
- [ ] Chat API å‚æ•°éªŒè¯æµ‹è¯•
- [ ] Audio API æ–°åŠŸèƒ½æµ‹è¯•
- [ ] Images API æ¨¡å‹æ”¯æŒæµ‹è¯•
- [ ] åºåˆ—åŒ–/ååºåˆ—åŒ–æµ‹è¯•

### é˜¶æ®µ 2: é›†æˆæµ‹è¯• (1 å‘¨)
- [ ] çœŸå® API è°ƒç”¨æµ‹è¯•
- [ ] é”™è¯¯å¤„ç†æµ‹è¯•
- [ ] è¾¹ç•Œæ¡ä»¶æµ‹è¯•
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•

### é˜¶æ®µ 3: åˆè§„æ€§æµ‹è¯• (1 å‘¨)
- [ ] OpenAPI è§„èŒƒéªŒè¯
- [ ] å“åº”æ ¼å¼éªŒè¯
- [ ] å‚æ•°èŒƒå›´éªŒè¯
- [ ] æ¨¡å‹å…¼å®¹æ€§æµ‹è¯•

### é˜¶æ®µ 4: å›å½’æµ‹è¯• (æŒç»­)
- [ ] ç°æœ‰åŠŸèƒ½éªŒè¯
- [ ] å‘åå…¼å®¹æ€§æµ‹è¯•
- [ ] è‡ªåŠ¨åŒ–æµ‹è¯•å¥—ä»¶
- [ ] CI/CD é›†æˆ

---

*æœ¬æµ‹è¯•è®¡åˆ’ç¡®ä¿æ‰€æœ‰æ–°åŠŸèƒ½éƒ½ç»è¿‡å……åˆ†éªŒè¯ï¼Œå¹¶ä¸ OpenAI API è§„èŒƒå®Œå…¨å…¼å®¹ã€‚*
